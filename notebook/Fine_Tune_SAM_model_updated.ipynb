{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/SAM/Fine_tune_SAM_(segment_anything)_on_a_custom_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAZ7M__ncb8Y"
   },
   "source": [
    "# Notebook: fine-tune SAM (segment anything) on a custom dataset\n",
    "\n",
    "In this notebook, we'll reproduce the [MedSAM](https://github.com/bowang-lab/MedSAM) project, which fine-tunes [SAM](https://huggingface.co/docs/transformers/main/en/model_doc/sam) on a dataset of medical images. For demo purposes, we'll use a toy dataset, but this can easily be scaled up.\n",
    "\n",
    "Resources used to create this notebook (thanks \ud83d\ude4f):\n",
    "* [Encode blog post](https://encord.com/blog/learn-how-to-fine-tune-the-segment-anything-model-sam/)\n",
    "* [MedSAM repository](https://github.com/bowang-lab/MedSAM).\n",
    "\n",
    "## Set-up environment\n",
    "\n",
    "We first install \ud83e\udd17 Transformers and \ud83e\udd17 Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwsv4JwZcV8c"
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Vdl2xr6cgs8"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhFPJgpIbVat"
   },
   "source": [
    "We also install the [Monai](https://github.com/Project-MONAI/MONAI) repository as we'll use a custom loss function from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XiRBGNGlV4Ph"
   },
   "outputs": [],
   "source": [
    "!pip install -q monai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROd15m4Ucdut"
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "Here we load a small dataset of 130 (image, ground truth mask) pairs.\n",
    "\n",
    "To load your own images and masks, refer to the bottom of my [SAM inference notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SAM/Run_inference_with_MedSAM_using_HuggingFace_Transformers.ipynb).\n",
    "\n",
    "See also [this guide](https://huggingface.co/docs/datasets/image_dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAaCCLBIGRGW"
   },
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "from google.colab import files\n",
    "files.upload()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qadrcU9fG51d"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88c3idsRG9lb"
   },
   "outputs": [],
   "source": [
    "!chmod 600 ~/.kaggle/kaggle.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ko7KbtWEHQfh"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d xhlulu/panda-resized-train-data-512x512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLqUe8o4HtfX"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Replace 'panda-resized-train-data-512x512.zip' with the name of the zip file you downloaded\n",
    "with zipfile.ZipFile('panda-resized-train-data-512x512.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./panda_resized_train_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1JjJ6jfIH9Y"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "# Open the image\n",
    "img = Image.open('/content/panda_resized_train_data/train_images/train_images/0005f7aaab2800f6170c399693a96917.png')\n",
    "\n",
    "# Display the image using Matplotlib\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymqLXuwXJkoj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gc  # for clearing memory\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define paths to image and mask folders\n",
    "image_folder = \"/content/panda_resized_train_data/train_images/train_images\"\n",
    "mask_folder = \"/content/panda_resized_train_data/train_label_masks/train_label_masks\"\n",
    "\n",
    "# Output directories to save patchified images\n",
    "output_img_dir = \"/content/patchified_data/images\"\n",
    "output_mask_dir = \"/content/patchified_data/masks\"\n",
    "os.makedirs(output_img_dir, exist_ok=True)\n",
    "os.makedirs(output_mask_dir, exist_ok=True)\n",
    "\n",
    "# Define patch size and step\n",
    "patch_size = 256\n",
    "step = 256\n",
    "\n",
    "def patchify_and_save(image, mask, image_name):\n",
    "    \"\"\"\n",
    "    Create non-overlapping patches from image and mask,\n",
    "    and save them individually to disk.\n",
    "    \"\"\"\n",
    "    # Pad image and mask to be divisible by patch_size\n",
    "    def pad(arr):\n",
    "        h, w = arr.shape[:2]\n",
    "        pad_h = (patch_size - (h % patch_size)) % patch_size\n",
    "        pad_w = (patch_size - (w % patch_size)) % patch_size\n",
    "        if arr.ndim == 3:\n",
    "            return np.pad(arr, ((0, pad_h), (0, pad_w), (0, 0)), mode='constant')\n",
    "        else:\n",
    "            return np.pad(arr, ((0, pad_h), (0, pad_w)), mode='constant')\n",
    "\n",
    "    image_padded = pad(image)\n",
    "    mask_padded = pad(mask)\n",
    "\n",
    "    # Extract and save patches\n",
    "    h, w = image_padded.shape[:2]\n",
    "    patch_index = 0\n",
    "    for y in range(0, h, step):\n",
    "        for x in range(0, w, step):\n",
    "            patch_img = image_padded[y:y+patch_size, x:x+patch_size]\n",
    "            patch_mask = mask_padded[y:y+patch_size, x:x+patch_size]\n",
    "\n",
    "            img_patch_path = os.path.join(output_img_dir, f\"{image_name}_{patch_index}.png\")\n",
    "            mask_patch_path = os.path.join(output_mask_dir, f\"{image_name}_{patch_index}.png\")\n",
    "\n",
    "            Image.fromarray(patch_img.astype(np.uint8)).save(img_patch_path)\n",
    "            Image.fromarray(patch_mask.astype(np.uint8)).save(mask_patch_path)\n",
    "            patch_index += 1\n",
    "\n",
    "# Get file names\n",
    "image_files = sorted(os.listdir(image_folder))\n",
    "mask_files = sorted(os.listdir(mask_folder))\n",
    "\n",
    "# Loop over files and process one pair at a time\n",
    "for img_file, mask_file in tqdm(zip(image_files, mask_files), total=len(image_files)):\n",
    "    img_path = os.path.join(image_folder, img_file)\n",
    "    mask_path = os.path.join(mask_folder, mask_file)\n",
    "\n",
    "    image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "    mask = np.array(Image.open(mask_path).convert(\"L\"))  # grayscale mask\n",
    "\n",
    "    image_name = os.path.splitext(img_file)[0]\n",
    "    patchify_and_save(image, mask, image_name)\n",
    "\n",
    "    # Free memory\n",
    "    del image, mask\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\u2705 Patchification completed. Files saved in:\")\n",
    "print(f\"  Images -> {output_img_dir}\")\n",
    "print(f\"  Masks  -> {output_mask_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2cOa80Qc3FQ"
   },
   "source": [
    "## Create PyTorch dataset\n",
    "\n",
    "Below we define a regular PyTorch dataset, which gives us examples of the data prepared in the format for the model. Each example consists of:\n",
    "\n",
    "* pixel values (which is the image prepared for the model)\n",
    "* a prompt in the form of a bounding box\n",
    "* a ground truth segmentation mask.\n",
    "\n",
    "The function below defines how to get a bounding box prompt based on the ground truth segmentation. This was taken from [here](https://github.com/bowang-lab/MedSAM/blob/66cf4799a9ab9a8e08428a5087e73fc21b2b61cd/train.py#L29).\n",
    "\n",
    "Note that SAM is always trained using certain \"prompts\", which you could be bounding boxes, points, text, or rudimentary masks. The model is then trained to output the appropriate mask given the image + prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LWBu8d1egig"
   },
   "outputs": [],
   "source": [
    "def get_bounding_box(ground_truth_map):\n",
    "    # Get nonzero pixel indices\n",
    "    y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "\n",
    "    # Handle empty mask safely\n",
    "    if len(x_indices) == 0 or len(y_indices) == 0:\n",
    "        H, W = ground_truth_map.shape\n",
    "        return [0, 0, W, H]\n",
    "\n",
    "    # Normal bounding box calculation\n",
    "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "\n",
    "    # Add small random perturbation\n",
    "    H, W = ground_truth_map.shape\n",
    "    x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "    x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "    y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "    y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "\n",
    "    return [x_min, y_min, x_max, y_max]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyC58ImHc2vO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Define your dataset class\n",
    "class CustomSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.mask_files = sorted(os.listdir(mask_dir))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n",
    "\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"))\n",
    "\n",
    "        # Optional: apply transforms (e.g., ToTensor, normalization)\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "\n",
    "        return {\"image\": image, \"mask\": mask}\n",
    "\n",
    "# \ud83d\udfe2 Initialize your dataset here\n",
    "train_dataset = CustomSegmentationDataset(\n",
    "    image_dir=\"/content/patchified_data/images\",\n",
    "    mask_dir=\"/content/patchified_data/masks\"\n",
    ")\n",
    "\n",
    "# Now your existing code will work fine\n",
    "sample = train_dataset[0]\n",
    "for k, v in sample.items():\n",
    "    print(k, type(v), getattr(v, \"shape\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_oIY2uS48YX"
   },
   "outputs": [],
   "source": [
    "sample = train_dataset[0]\n",
    "for k, v in sample.items():\n",
    "    print(k, type(v), getattr(v, \"shape\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "150600c5"
   },
   "source": [
    "from transformers import SamProcessor\n",
    "\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bacc5801"
   },
   "source": [
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def get_bounding_box(ground_truth_map):\n",
    "    # Get nonzero pixel indices\n",
    "    y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "\n",
    "    # Handle empty mask safely\n",
    "    if len(x_indices) == 0 or len(y_indices) == 0:\n",
    "        H, W = ground_truth_map.shape\n",
    "        return [0, 0, W, H]\n",
    "\n",
    "    # Normal bounding box calculation\n",
    "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "\n",
    "    # Add small random perturbation\n",
    "    H, W = ground_truth_map.shape\n",
    "    x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "    x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "    y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "    y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "\n",
    "    return [x_min, y_min, x_max, y_max]\n",
    "\n",
    "class SAMDataset(Dataset):\n",
    "  def __init__(self, dataset, processor):\n",
    "    self.dataset = dataset\n",
    "    self.processor = processor\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    image_path, mask_path = self.dataset[idx]\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    ground_truth_mask = np.array(Image.open(mask_path).convert(\"L\"))\n",
    "\n",
    "    # get bounding box prompt from mask\n",
    "    prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "    # prepare image and prompt for the model\n",
    "    inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "    # remove batch dimension created by the processor\n",
    "    inputs = {\n",
    "k:v.squeeze(0) if k != \"original_sizes\" else v.squeeze(0) for k,v in inputs.items()\n",
    "}\n",
    "\n",
    "    # add ground truth segmentation\n",
    "    inputs[\"ground_truth_mask\"] = torch.tensor(ground_truth_mask, dtype=torch.long)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "# 1\ufe0f\u20e3 Get file paths for all image-mask pairs\n",
    "image_paths = sorted(glob.glob(\"/content/patchified_data/images/*.png\"))\n",
    "mask_paths = sorted(glob.glob(\"/content/patchified_data/masks/*.png\"))\n",
    "\n",
    "# 2\ufe0f\u20e3 Zip them together into a dataset list\n",
    "dataset = list(zip(image_paths, mask_paths))\n",
    "\n",
    "# 3\ufe0f\u20e3 (Optional) Quick sanity check\n",
    "print(\"Total patches found:\", len(dataset))\n",
    "print(\"Example pair:\\n\", dataset[0])\n",
    "\n",
    "# 4\ufe0f\u20e3 Now create your SAMDataset object\n",
    "train_dataset = SAMDataset(dataset=dataset, processor=processor)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4615e9dd"
   },
   "source": [
    "sample = train_dataset[0]\n",
    "for k, v in sample.items():\n",
    "    print(k, type(v), getattr(v, \"shape\", None))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wE4iTOZdeLjq"
   },
   "outputs": [],
   "source": [
    "from transformers import SamProcessor\n",
    "\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTynfgToe8jj"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# 1\ufe0f\u20e3 Get file paths for all image-mask pairs\n",
    "image_paths = sorted(glob.glob(\"/content/patchified_data/images/*.png\"))\n",
    "mask_paths = sorted(glob.glob(\"/content/patchified_data/masks/*.png\"))\n",
    "\n",
    "# 2\ufe0f\u20e3 Zip them together into a dataset list\n",
    "dataset = list(zip(image_paths, mask_paths))\n",
    "\n",
    "# 3\ufe0f\u20e3 (Optional) Quick sanity check\n",
    "print(\"Total patches found:\", len(dataset))\n",
    "print(\"Example pair:\\n\", dataset[0])\n",
    "\n",
    "# 4\ufe0f\u20e3 Now create your SAMDataset object\n",
    "train_dataset = SAMDataset(dataset=dataset, processor=processor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyD4kOCFf76Q"
   },
   "source": [
    "## Create PyTorch DataLoader\n",
    "\n",
    "Next we define a PyTorch Dataloader, which allows us to get batches from the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CUnLOjSf9Kn"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5D2bmjAhgIus"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJyuJc7fOldT"
   },
   "outputs": [],
   "source": [
    "batch[\"ground_truth_mask\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQx2Aq7LeAMU"
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZI2ioeS5eAxm"
   },
   "outputs": [],
   "source": [
    "from transformers import SamModel\n",
    "\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# make sure we only compute gradients for mask decoder\n",
    "for name, param in model.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CShhxC-heDpw"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOTrMv1LeEK9"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import monai\n",
    "\n",
    "# Note: Hyperparameter tuning could improve performance here\n",
    "optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
    "\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYKvlFthRsyG"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Important note here: as we used the Dice loss with `sigmoid=True`, we need to make sure to appropriately apply a sigmoid activation function to the predicted masks. Hence we won't use the processor's `post_process_masks` method here."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Robust training loop with safe checkpoints, logging, and interruption handling.\n",
    "# This cell replaces the original training loop. It will:\n",
    "# - create OUTPUT_DIR (\"/content/outputs\")\n",
    "# - save training_config.json and model_config.yaml (minimal) for reproducibility\n",
    "# - log training progress to training_log.txt\n",
    "# - save periodic backup checkpoints and epoch checkpoints\n",
    "# - save an \"interrupted\" model if manually stopped with Ctrl+C (KeyboardInterrupt)\n",
    "# - save a loss curve (loss_curve.png) at the end or on interruption\n",
    "# - ensure minimal additional imports are present in this cell so it runs standalone\n",
    "\n",
    "import os, json, datetime, traceback\n",
    "from statistics import mean\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ------------------------------\n",
    "# OUTPUT / CONFIG SETUP\n",
    "# ------------------------------\n",
    "OUTPUT_DIR = \"/content/outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Basic training config (will overwrite if variables exist above)\n",
    "training_config = {\n",
    "    \"num_epochs\": globals().get(\"num_epochs\", 50),\n",
    "    \"save_every_batches\": globals().get(\"save_every_batches\", 200),\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"batch_size\": None,\n",
    "    \"timestamp\": datetime.datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Try to capture batch size if available from dataloader\n",
    "try:\n",
    "    sample = next(iter(train_dataloader))\n",
    "    if \"pixel_values\" in sample:\n",
    "        training_config[\"batch_size\"] = sample[\"pixel_values\"].shape[0]\n",
    "except Exception:\n",
    "    training_config[\"batch_size\"] = training_config.get(\"batch_size\", None)\n",
    "\n",
    "# Save training_config.json\n",
    "with open(os.path.join(OUTPUT_DIR, \"training_config.json\"), \"w\") as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "# Minimal model config (you may edit manually later)\n",
    "model_config = getattr(model, \"config\", None)\n",
    "try:\n",
    "    model_config_to_save = model_config if model_config is not None else {\"model_type\": type(model).__name__}\n",
    "except Exception:\n",
    "    model_config_to_save = {\"model_type\": type(model).__name__}\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"model_config.yaml\"), \"w\") as f:\n",
    "    # simple YAML-ish dump without requiring pyyaml\n",
    "    f.write(\"# Minimal model config (auto-generated)\\n\")\n",
    "    for k, v in model_config_to_save.items() if isinstance(model_config_to_save, dict) else []:\n",
    "        f.write(f\"{k}: {v}\\n\")\n",
    "\n",
    "# Setup logging file\n",
    "log_path = os.path.join(OUTPUT_DIR, \"training_log.txt\")\n",
    "log_f = open(log_path, \"a\", buffering=1)  # line-buffered\n",
    "\n",
    "def log(msg):\n",
    "    ts = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    line = f\"[{ts}] {msg}\"\n",
    "    print(line)\n",
    "    try:\n",
    "        log_f.write(line + \"\\\\n\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "log(\"STARTING TRAINING (robust loop)\")\n",
    "log(f\"Config: {training_config}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Hyperparams (use existing variables if present)\n",
    "# ------------------------------\n",
    "num_epochs = globals().get(\"num_epochs\", training_config[\"num_epochs\"])\n",
    "save_every_batches = globals().get(\"save_every_batches\", training_config[\"save_every_batches\"])\n",
    "device = training_config[\"device\"]\n",
    "model.to(device)\n",
    "\n",
    "# For plotting loss curve\n",
    "all_epoch_means = []\n",
    "\n",
    "# Track total saved checkpoints count for user question later\n",
    "saved_checkpoints = []\n",
    "\n",
    "# ------------------------------\n",
    "# TRAINING\n",
    "# ------------------------------\n",
    "try:\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        log(f\"Starting Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        for batch_idx, batch in enumerate(tqdm(train_dataloader, desc=f\\\"Epoch {epoch + 1}\\\")):\n",
    "\n",
    "            # ---------- Prepare inputs ----------\n",
    "            pixel_values = batch.get(\"pixel_values\", None)\n",
    "            if pixel_values is None:\n",
    "                raise ValueError(\"Batch does not contain 'pixel_values' key. Please check dataloader.\")\n",
    "\n",
    "            if pixel_values.dim() == 5:\n",
    "                pixel_values = pixel_values.squeeze(1)\n",
    "\n",
    "            pixel_values = pixel_values.to(device)\n",
    "            input_boxes = batch.get(\"input_boxes\", None)\n",
    "            if input_boxes is not None:\n",
    "                input_boxes = input_boxes.to(device)\n",
    "\n",
    "            ground_truth_masks = batch.get(\"ground_truth_mask\", None)\n",
    "            if ground_truth_masks is None:\n",
    "                raise ValueError(\"Batch does not contain 'ground_truth_mask' key.\")\n",
    "\n",
    "            ground_truth_masks = ground_truth_masks.float().to(device)\n",
    "            if ground_truth_masks.dim() == 3:\n",
    "                ground_truth_masks = ground_truth_masks.unsqueeze(1)\n",
    "\n",
    "            # ---------- Forward pass ----------\n",
    "            outputs = model(\n",
    "                pixel_values=pixel_values,\n",
    "                input_boxes=input_boxes,\n",
    "                multimask_output=False\n",
    "            )\n",
    "\n",
    "            predicted_masks = outputs.pred_masks\n",
    "\n",
    "            if predicted_masks.dim() == 5:\n",
    "                predicted_masks = predicted_masks.squeeze(1)\n",
    "            elif predicted_masks.dim() == 3:\n",
    "                predicted_masks = predicted_masks.unsqueeze(1)\n",
    "\n",
    "            if predicted_masks.shape[-2:] != ground_truth_masks.shape[-2:]:\n",
    "                predicted_masks = F.interpolate(\n",
    "                    predicted_masks,\n",
    "                    size=ground_truth_masks.shape[-2:],\n",
    "                    mode='bilinear',\n",
    "                    align_corners=False\n",
    "                )\n",
    "\n",
    "            # ---------- Loss & Backprop ----------\n",
    "            loss = seg_loss(predicted_masks, ground_truth_masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            # ---------- Periodic backup ----------\n",
    "            if (batch_idx + 1) % save_every_batches == 0:\n",
    "                ckpt_path = os.path.join(OUTPUT_DIR, f\"backup_epoch{epoch+1}_batch{batch_idx+1}.pt\")\n",
    "                torch.save(model.state_dict(), ckpt_path)\n",
    "                saved_checkpoints.append(ckpt_path)\n",
    "                log(f\\\"Saved periodic backup: {ckpt_path} -- loss: {loss.item():.6f}\\\")\n",
    "\n",
    "            # Optional: log progress each N batches to console/file\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                log(f\\\"Batch {batch_idx+1} | Batch Loss: {loss.item():.6f}\\\")\n",
    "\n",
    "        # ---------- End of epoch ----------\n",
    "        epoch_ckpt = os.path.join(OUTPUT_DIR, f\"model_epoch_{epoch+1}.pt\")\n",
    "        torch.save(model.state_dict(), epoch_ckpt)\n",
    "        saved_checkpoints.append(epoch_ckpt)\n",
    "        log(f\\\"Saved epoch checkpoint \u2192 {epoch_ckpt}\\\")\n",
    "\n",
    "        mean_loss = mean(epoch_losses) if len(epoch_losses) else float('nan')\n",
    "        all_epoch_means.append(mean_loss)\n",
    "        log(f\\\"EPOCH {epoch+1} COMPLETED | Mean Loss: {mean_loss:.6f}\\\")\n",
    "\n",
    "    # Save final model\n",
    "    final_path = os.path.join(OUTPUT_DIR, \"model_final.pt\")\n",
    "    torch.save(model.state_dict(), final_path)\n",
    "    saved_checkpoints.append(final_path)\n",
    "    log(f\\\"Training finished. Saved final model \u2192 {final_path}\\\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Manual stop by user (Ctrl+C). Save an interrupted checkpoint.\n",
    "    interrupted_path = os.path.join(OUTPUT_DIR, \"interrupted_model.pt\")\n",
    "    try:\n",
    "        torch.save(model.state_dict(), interrupted_path)\n",
    "        saved_checkpoints.append(interrupted_path)\n",
    "        log(\\\"Manual KeyboardInterrupt received. Saved interrupted model.\\\")\n",
    "        log(f\\\"Saved at: {interrupted_path}\\\")\n",
    "    except Exception as ex:\n",
    "        log(f\\\"Failed saving interrupted checkpoint: {ex}\\\")\n",
    "    # Still re-raise so the notebook cell shows interruption unless you want to suppress\n",
    "    raise\n",
    "\n",
    "except Exception as e:\n",
    "    # Unexpected crash: save emergency checkpoint and the traceback\n",
    "    emergency_path = os.path.join(OUTPUT_DIR, \"crashed_model.pt\")\n",
    "    try:\n",
    "        torch.save(model.state_dict(), emergency_path)\n",
    "        saved_checkpoints.append(emergency_path)\n",
    "        log(\\\"ERROR occurred during training. Saved emergency checkpoint.\\\")\n",
    "        log(f\\\"Saved at: {emergency_path}\\\")\n",
    "    except Exception as ex:\n",
    "        log(f\\\"Failed saving emergency checkpoint: {ex}\\\")\n",
    "    log(\\\"Traceback:\\\")\n",
    "    log(traceback.format_exc())\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    # Close log file safely\n",
    "    try:\n",
    "        log_f.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Save loss curve if we have epoch means\n",
    "    try:\n",
    "        if len(all_epoch_means) > 0:\n",
    "            plt.figure()\n",
    "            plt.plot(range(1, len(all_epoch_means) + 1), all_epoch_means)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Mean Loss')\n",
    "            plt.title('Training Loss Curve (mean loss per epoch)')\n",
    "            loss_png = os.path.join(OUTPUT_DIR, 'loss_curve.png')\n",
    "            plt.savefig(loss_png, bbox_inches='tight')\n",
    "            log(f\\\"Saved loss curve \u2192 {loss_png}\\\")\n",
    "    except Exception as ex:\n",
    "        print(\\\"Failed saving loss curve:\\\", ex)\n",
    "\n",
    "    # Write a small results_summary.md\n",
    "    try:\n",
    "        summary_path = os.path.join(OUTPUT_DIR, \\\"results_summary.md\\\")\n",
    "        with open(summary_path, \\\"w\\\") as sf:\n",
    "            sf.write(\\\"# Training Results Summary\\\\n\\\\n\\\")\n",
    "            sf.write(f\\\"**Timestamp:** {datetime.datetime.now().isoformat()}\\\\n\\\\n\\\")\n",
    "            sf.write(f\\\"**Saved checkpoints:**\\\\n\\\\n\\\") \n",
    "            for ck in saved_checkpoints:\n",
    "                sf.write(f\\\"- {ck}\\\\n\\\")\n",
    "            sf.write(\\\"\\\\n**Notes:** Automatic summary created by notebook.\\\\n\\\")\n",
    "    except Exception as ex:\n",
    "        print(\\\"Failed writing summary:\\\", ex)\n",
    "\n",
    "    log(\\\"CLEANUP DONE. Check the outputs folder for saved artifacts.\\\")\n"
   ],
   "metadata": {
    "id": "HSLik9d5AX_q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyES0ru3aiVf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# let's take a random training example\n",
    "idx = 10\n",
    "\n",
    "# load image\n",
    "image = dataset[idx][\"image\"]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzxIzE6janRx"
   },
   "outputs": [],
   "source": [
    "# get box prompt based on ground truth segmentation map\n",
    "ground_truth_mask = np.array(dataset[idx][\"label\"])\n",
    "prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "# prepare image + box prompt for the model\n",
    "inputs = processor(image, input_boxes=[[prompt]], return_tensors=\"pt\").to(device)\n",
    "for k,v in inputs.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvOVzNvea5oU"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "  outputs = model(**inputs, multimask_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvgwHg42bACQ"
   },
   "outputs": [],
   "source": [
    "# apply sigmoid\n",
    "medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "# convert soft mask to hard mask\n",
    "medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
    "medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCehzlJJbDGz"
   },
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "\n",
    "axes.imshow(np.array(image))\n",
    "show_mask(medsam_seg, axes)\n",
    "axes.title.set_text(f\"Predicted mask\")\n",
    "axes.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFbiFDAabLNg"
   },
   "source": [
    "Compare this to the ground truth segmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEISRU0WbEvg"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots()\n",
    "\n",
    "axes.imshow(np.array(image))\n",
    "show_mask(ground_truth_mask, axes)\n",
    "axes.title.set_text(f\"Ground truth mask\")\n",
    "axes.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkKnp-mtRyZl"
   },
   "source": [
    "## Legacy\n",
    "\n",
    "The code below was used during the creation of this notebook, but was eventually not used anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eA6SSA_GR4dU"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "loss_fn = MSELoss()\n",
    "\n",
    "def postprocess_masks(masks: torch.Tensor, input_size: Tuple[int, ...], original_size: Tuple[int, ...], image_size=1024) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Remove padding and upscale masks to the original image size.\n",
    "\n",
    "    Args:\n",
    "      masks (torch.Tensor):\n",
    "        Batched masks from the mask_decoder, in BxCxHxW format.\n",
    "      input_size (tuple(int, int)):\n",
    "        The size of the image input to the model, in (H, W) format. Used to remove padding.\n",
    "      original_size (tuple(int, int)):\n",
    "        The original size of the image before resizing for input to the model, in (H, W) format.\n",
    "\n",
    "    Returns:\n",
    "      (torch.Tensor): Batched masks in BxCxHxW format, where (H, W)\n",
    "        is given by original_size.\n",
    "    \"\"\"\n",
    "    masks = F.interpolate(\n",
    "        masks,\n",
    "        (image_size, image_size),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    masks = masks[..., : input_size[0], : input_size[1]]\n",
    "    masks = F.interpolate(masks, original_size, mode=\"bilinear\", align_corners=False)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuYtdSNFRy64"
   },
   "outputs": [],
   "source": [
    "# upscaled_masks = postprocess_masks(low_res_masks.squeeze(1), batch[\"reshaped_input_sizes\"][0].tolist(), batch[\"original_sizes\"][0].tolist()).to(device)\n",
    "# predicted_masks = normalize(threshold(upscaled_masks, 0.0, 0)).squeeze(1)\n",
    "# loss = loss_fn(predicted_masks, ground_truth_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FEs58oPbNTv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}